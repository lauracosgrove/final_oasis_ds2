---
title: "gbm"
author: "Laura Cosgrove"
date: "5/7/2019"
output: html_document
---

```{r}
library(tidyverse)
library(caret)
library(ranger)
library(gbm)
```


```{r}
# Using caret
ctrl1 <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary, #because we're in the two-class setting
                     classProbs = TRUE) #because need predicted class probabilities to get ROC curve

#Read RDS 
cog_train <- readRDS("./data/cog_train_preproc.RDS")
cog_test <- readRDS("./data/cog_test_preproc.RDS")
```


```{r}
set.seed(1)

#tuning 
gbm_grid <- expand.grid(n.trees = c(2000,3000),
                        interaction.depth = 2:10,
                        shrinkage = c(0.01, 0.03, 0.05),
                        n.minobsinnode = 1)

gbm_fit <- train(x = cog_train[3:10],
                 y = cog_train$cdr,
                 method = "gbm",
                 tuneGrid = gbm_grid,
                 trControl = ctrl1,
                 verbose = FALSE)

ggplot(gbm_fit, highlight = TRUE) 

saveRDS(gbm_fit, file = "./data/gbm_fit_1.RDS")

gbm_fit$results[which.max(gbm_fit$results$ROC),]

```

```{r}
set.seed(12)

gbm_grid_2 <- expand.grid(n.trees = 2000,
                        interaction.depth = 2:8,
                        shrinkage = c(0.0008, 0.001, 0.004),
                        n.minobsinnode = 1)

gbm_fit_2 <- train(x = cog_train[3:10],
                 y = cog_train$cdr,
                 method = "gbm",
                 tuneGrid = gbm_grid_2,
                 trControl = ctrl1,
                 verbose = FALSE)

ggplot(gbm_fit_2, highlight = TRUE) 

saveRDS(gbm_fit_2, file = "./data/gbm_fit_2.RDS")

gbm_fit_2$results[which.max(gbm_fit_2$results$ROC),]


```

```{r}
set.seed(12)

gbm_grid_3 <- expand.grid(n.trees = c(2000, 5000),
                        interaction.depth = 4:10,
                        shrinkage = 0.001,
                        n.minobsinnode = 1)

gbm_fit_3 <- train(x = cog_train[3:10],
                 y = cog_train$cdr,
                 method = "gbm",
                 tuneGrid = gbm_grid_3,
                 trControl = ctrl1,
                 verbose = FALSE)

ggplot(gbm_fit_3, highlight = TRUE) 

saveRDS(gbm_fit_3, file = "./data/gbm_fit_3.RDS")

gbm_fit_3$results[which.max(gbm_fit_3$results$ROC),]
```

# xgboost

```{r}
library(xgboost)
```

```{r}
#preparing matrix

dtrain <- xgb.DMatrix(data = model.matrix(cdr ~., cog_train[2:10])[,-1], 
                      label = (as.numeric(cog_train$cdr) -1) ) 
dtest <- xgb.DMatrix(data = model.matrix(cdr ~., cog_test[2:10])[,-1],
                     label = (as.numeric(cog_test$cdr) -1) )

set.seed(4)
params <- list(booster = "gbtree", 
               objective = "binary:logistic", 
               eta=0.3, gamma=0, max_depth=6, 
               min_child_weight=1, subsample=1, 
               colsample_bytree=1)

xgbcv <- xgb.cv( params = params, 
                 data = dtrain, 
                 nrounds = 100, 
                 metrics = "auc",
                 nfold = 5, showsd = T, 
                 stratified = T)
xgbcv$evaluation_log

#first default - model training
xgb1 <- xgb.train (params = params, 
                   data = dtrain, 
                   nrounds = 100, 
                   watchlist = list(val=dtest,train=dtrain), 
                   print.every.n = 10, 
                   early.stop.round = 10, maximize = F , eval_metric = "auc")
#model prediction
xgbpred <- predict(xgb1, dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)

xgbpred <- factor(xgbpred, levels = c("0", "1"))
xgbpred <- forcats::fct_recode(xgbpred, "NonDementia" = "0", "Dementia" = "1")
            
confusionMatrix(xgbpred, reference = cog_test$cdr)



```

