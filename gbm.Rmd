---
title: "gbm"
author: "Laura Cosgrove"
date: "5/7/2019"
output: html_document
---

```{r}
library(tidyverse)
library(caret)
library(ranger)
library(gbm)
library(pROC)
```


```{r}
# Using caret
ctrl1 <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary, #because we're in the two-class setting
                     classProbs = TRUE) #because need predicted class probabilities to get ROC curve

#Read RDS 
cog_train <- readRDS("./data/cog_train_preproc.RDS")
cog_test <- readRDS("./data/cog_test_preproc.RDS")
```


```{r eval = FALSE}
set.seed(1)

#tuning 
gbm_grid <- expand.grid(n.trees = c(2000,3000),
                        interaction.depth = 2:10,
                        shrinkage = c(0.01, 0.03, 0.05),
                        n.minobsinnode = 1)

gbm_fit <- train(x = cog_train[3:10],
                 y = cog_train$cdr,
                 method = "gbm",
                 tuneGrid = gbm_grid,
                 trControl = ctrl1,
                 verbose = FALSE)
```


```{r}
#Save and reload
#saveRDS(gbm_fit, file = "./data/gbm_fit_1.RDS")
gbm_fit_1 = readRDS("./data/gbm_fit_1.RDS")

ggplot(gbm_fit_1, highlight = TRUE) 

gbm_fit_1$results[which.max(gbm_fit_1$results$ROC),]
```

```{r eval = FALSE}
set.seed(12)

gbm_grid_2 <- expand.grid(n.trees = 2000,
                        interaction.depth = 2:8,
                        shrinkage = c(0.0008, 0.001, 0.004),
                        n.minobsinnode = 1)

gbm_fit_2 <- train(x = cog_train[3:10],
                 y = cog_train$cdr,
                 method = "gbm",
                 tuneGrid = gbm_grid_2,
                 trControl = ctrl1,
                 verbose = FALSE)

```


```{r}
#Save and reload
#saveRDS(gbm_fit_2, file = "./data/gbm_fit_3.RDS")
gbm_fit_2 = readRDS("./data/gbm_fit_2.RDS")

ggplot(gbm_fit_2, highlight = TRUE) 

gbm_fit_2$results[which.max(gbm_fit_2$results$ROC),]

```

```{r  eval = FALSE}
set.seed(12)

gbm_grid_3 <- expand.grid(n.trees = c(2000, 5000),
                        interaction.depth = 4:10,
                        shrinkage = 0.001,
                        n.minobsinnode = 1)

gbm_fit_3 <- train(x = cog_train[3:10],
                 y = cog_train$cdr,
                 distribution = "bernoulli",
                 method = "gbm",
                 tuneGrid = gbm_grid_3,
                 trControl = ctrl1,
                 verbose = FALSE)
```


```{r}
#Save and reload
#saveRDS(gbm_fit_3, file = "./data/gbm_fit_3.RDS")
gbm_fit_3 = readRDS("./data/gbm_fit_3.RDS")

ggplot(gbm_fit_3, highlight = TRUE) 


gbm_fit_3$results[which.max(gbm_fit_3$results$ROC),]

## variable importance
summary.gbm(gbm_fit_3$finalModel)

```

Test predictions, using gbm model 3:

```{r}
##Test Predictions## 

pred_gbm_raw <- predict(gbm_fit_3, newdata = cog_test,
                    n.trees = 5000,
                    type = "raw")

confusionMatrix(data = pred_gbm_raw, 
                reference = cog_test$cdr,
                positive = "Dementia")

pred_gbm_prob <- predict(gbm_fit_3, newdata = cog_test,
                    n.trees = 5000,
                    type = "prob")

roc_gbm_test <- roc(cog_test$cdr, pred_gbm_prob$Dementia)

plot(roc_gbm_test, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc_gbm_test), col = 4, add = TRUE) 

```

# xgboost

```{r}
library(xgboost)
```

```{r}
#preparing matrix

dtrain <- xgb.DMatrix(data = model.matrix(cdr ~., cog_train[2:10])[,-1], 
                      label = (as.numeric(cog_train$cdr) -1) ) 
dtest <- xgb.DMatrix(data = model.matrix(cdr ~., cog_test[2:10])[,-1],
                     label = (as.numeric(cog_test$cdr) -1) )

set.seed(4)
params <- list(booster = "gbtree", 
               objective = "binary:logistic", 
               eta=0.3, gamma=0, max_depth=6, 
               min_child_weight=1, subsample=1, 
               colsample_bytree=1)

xgbcv <- xgb.cv( params = params, 
                 data = dtrain, 
                 nrounds = 100, 
                 metrics = "auc",
                 nfold = 5, showsd = T, 
                 stratified = T)
xgbcv$evaluation_log

#first default - model training
xgb1 <- xgb.train(params = params, 
                   data = dtrain, 
                   nrounds = 100, 
                   watchlist = list(val=dtest,train=dtrain), 
                   print.every.n = 10, 
                   early.stop.round = 10, maximize = F , eval_metric = "auc")

#model prediction on test data
xgbpred_prob <- predict(xgb1, dtest)
xgbpred <- ifelse (xgbpred_prob > 0.5,1,0)

xgbpred <- factor(xgbpred, levels = c("0", "1"))
xgbpred <- forcats::fct_recode(xgbpred, "NonDementia" = "0", "Dementia" = "1")
            
confusionMatrix(xgbpred, reference = cog_test$cdr)

#roc test
roc_xboost_test <- roc(cog_test$cdr, xgbpred_prob)

plot(roc_xboost_test, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc_xboost_test), col = 4, add = TRUE) 

#roc train
xgbpred_train_prob <- predict(xgb1, dtrain)

roc_xboost_train <- roc(cog_train$cdr, xgbpred_train_prob)

plot(roc_xboost_train, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc_xboost_train), col = 4, add = TRUE) 


#importance
mat <- xgb.importance(feature_names = colnames(cog_train[3:10]),
                      model = xgb1)
xgb.plot.importance(importance_matrix = mat) 


```

